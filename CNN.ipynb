{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch\n",
    "# 一些常量\n",
    "embedding_size = 100\n",
    "dir_train = 'D:/Github/candidate_answer/data/json_train_expt_stop'\n",
    "dir_test = 'D:/Github/candidate_answer/data/json_test_expt_stop'\n",
    "dir_embedding = 'D:/nlp_data/sogou_100_nobinary'\n",
    "max_question_words = 23 # 问题最大词数，下同理\n",
    "max_right_answer_words = 351\n",
    "max_wrong_answer_words = 824\n",
    "kernel_size = (3, embedding_size) # 卷积核的size\n",
    "out_channels = 300 # 输出通道数\n",
    "hidden_out = 400 # 隐藏层输出单元数\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading enmbedding...\n",
      "finish loading\n"
     ]
    }
   ],
   "source": [
    "# 加载词向量\n",
    "embedding = {}\n",
    "f = open(dir_embedding,\"r\",encoding='utf-8')\n",
    "line = f.readline()\n",
    "line_num = 0\n",
    "print(\"loading enmbedding...\")\n",
    "while line:\n",
    "    try:\n",
    "        content = line.strip(' \\n').split(' ')\n",
    "        assert len(content) == embedding_size + 1\n",
    "        embedding[content[0]] = np.array([float(i) for i in content[1:]])\n",
    "        line = f.readline()\n",
    "        line_num+=1\n",
    "#         print(line_num)\n",
    "    except:\n",
    "        print(content)\n",
    "        break\n",
    "print(\"finish loading\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建一个CNN\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 300, (3,100)) # 保证输出列向量在高度上与X相同\n",
    "        self.conv2 = nn.Conv2d(1, 300, (3,100))\n",
    "        self.conv3 = nn.Conv2d(1, 300, (3,100))\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(1, max_wrong_answer_words) # 输出是out_channels*1维向量\n",
    "        self.pool2 = nn.MaxPool2d(1, max_question_words)\n",
    "        self.pool3 = nn.MaxPool2d(1, max_right_answer_words)\n",
    "\n",
    "        self.fc1 = nn.Linear(out_channels, hidden_out)\n",
    "        self.fc2 = nn.Linear(out_channels, hidden_out)\n",
    "        self.fc3 = nn.Linear(out_channels, hidden_out)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        # x1/x2/x3 分别表示错误答案，问题，正确答案\n",
    "#         x1 = self.pool1(F.tanh(self.conv1(x1)))\n",
    "#         x2 = self.pool2(F.tanh(self.conv2(x2)))\n",
    "#         x3 = self.pool3(F.tanh(self.conv3(x3)))\n",
    "        print(\"in forward:\")\n",
    "        print(\"X:\",x1.size(),x2.size(),x3.size())\n",
    "        x1 = F.tanh(self.conv1(x1))\n",
    "        x2 = F.tanh(self.conv2(x2))\n",
    "        x3 = F.tanh(self.conv3(x3))\n",
    "        print(\"conv1:\",x1.size(),x2.size(),x3.size())\n",
    "        \n",
    "        x1 = self.pool1(x1)\n",
    "        x2 = self.pool2(x2)\n",
    "        x3 = self.pool3(x3)\n",
    "        print(\"pool:\",x1.size(),x2.size(),x3.size())\n",
    "        \n",
    "        x1 = F.tanh(x1)\n",
    "        x2 = F.tanh(x2)\n",
    "        x3 = F.tanh(x3)\n",
    "\n",
    "        neg_cosine = F.cosine_similarity(x1,x2)\n",
    "        pos_cosine = F.cosine_similarity(x2,x3)\n",
    "\n",
    "        return F.hinge_embedding_loss([neg_cosine,pos_cosine],size_average=False)\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "in forward:\n",
      "X: torch.Size([4, 1, 824, 100]) torch.Size([4, 1, 23, 100]) torch.Size([4, 1, 351, 100])\n",
      "conv1: torch.Size([4, 300, 822, 1]) torch.Size([4, 300, 21, 1]) torch.Size([4, 300, 349, 1])\n",
      "pool: torch.Size([4, 300, 1, 1]) torch.Size([4, 300, 1, 1]) torch.Size([4, 300, 1, 1])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn.functional' has no attribute 'hinge_embedding_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-53e8edae82f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;31m# forward + backward + optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a4af895f935c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x1, x2, x3)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mpos_cosine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhinge_embedding_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mneg_cosine\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos_cosine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.nn.functional' has no attribute 'hinge_embedding_loss'"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "def get_sentence_embedding(s,out_size):\n",
    "    arr = []\n",
    "    for word in s:\n",
    "        if word in embedding:\n",
    "            arr.append(embedding[word])\n",
    "        else:\n",
    "            arr.append([random.uniform(-1,1) for i in range(embedding_size)])\n",
    "    if len(arr) < out_size: # 补零\n",
    "        append_arr = [0 for i in range(embedding_size)]\n",
    "        for j in range(out_size - len(arr)):\n",
    "            arr.append(append_arr)\n",
    "    return [arr]\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "f = open(dir_train,'r',encoding='utf-8')\n",
    "data = json.loads(f.read())\n",
    "count_step = 0\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    f = open(dir_train)\n",
    "    running_loss = 0.0\n",
    "    batch = [[] for i in range(3)]\n",
    "    for id in data:\n",
    "        # get the inputs\n",
    "        x2 = get_sentence_embedding(data[id]['question'], max_question_words)\n",
    "        x3 = get_sentence_embedding(data[id]['right_answer'][0], max_right_answer_words)\n",
    "        for wrong_answer in data[id]['wrong_answer']:\n",
    "            x1 = get_sentence_embedding(wrong_answer, max_wrong_answer_words)\n",
    "            batch[0].append(x1)\n",
    "            batch[1].append(x2)\n",
    "            batch[2].append(x3)\n",
    "            if len(batch[0]) == batch_size:\n",
    "                print(np.array(batch).size)\n",
    "                # wrap them in Variable\n",
    "                x1, x2, x3 = Variable(torch.from_numpy(np.array(batch[0])).float()), Variable(torch.from_numpy(np.array(batch[1])).float()), Variable(torch.from_numpy(np.array(batch[2])).float())\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward + backward + optimize\n",
    "                loss = net(x1, x2, x3)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                count_step += 1\n",
    "                running_loss += loss.data[0]\n",
    "                if count_step % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.3f' %\n",
    "                          (epoch + 1, count_step + 1, running_loss / 2000))\n",
    "                    running_loss = 0.0\n",
    "                # clear batch\n",
    "                batch = [[] for i in range(2)]\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
